# robots.txt - SEO configuration for Eudora Delivery
# This file helps search engines understand which pages to crawl and index

# Allow all crawlers
User-agent: *
Allow: /
Disallow: /private/
Disallow: /admin/
Disallow: /.netlify/
Disallow: /node_modules/

# Specific rules for common bots
User-agent: Googlebot
Allow: /
Crawl-delay: 0

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 1

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

# Disallow AI crawlers that don't respect robots.txt guidelines (optional)
# Uncomment if you want to limit AI training data scraping
# User-agent: GPTBot
# Disallow: /
# User-agent: CCBot
# Disallow: /

# Sitemap location
Sitemap: https://eudora-delivery.netlify.app/sitemap.xml
